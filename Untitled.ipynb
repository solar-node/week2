{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4b25c9-b7bd-46b8-ba38-747e8394b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d70eb8a4-53f2-487b-a679-cb56f16b8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "gemini1 = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-lite',\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.getenv('GEMINI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928699a0-b102-46e9-babb-fd510e6fa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light hearted joke for an audience of Data Scientist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df07fe76-5172-41d4-b2b8-c1517454e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they had no *correlation*!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=user_prompt)\n",
    "]\n",
    "\n",
    "response = gemini1.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "629c2884-4721-4e0f-8c3b-30237469c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini2 = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.getenv('GEMINI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cd98c6c-4024-4ff4-ab7a-f36d4ed6df06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m messages = [\n\u001b[32m      2\u001b[39m     SystemMessage(content=system_message),\n\u001b[32m      3\u001b[39m     HumanMessage(content=user_prompt)\n\u001b[32m      4\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = gemini2.invoke(messages)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    396\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    397\u001b[39m             stop=stop,\n\u001b[32m    398\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    400\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    401\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    402\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    403\u001b[39m             **kwargs,\n\u001b[32m    404\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     **kwargs: Any,\n\u001b[32m   1021\u001b[39m ) -> LLMResult:\n\u001b[32m   1022\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate_with_cache(\n\u001b[32m    841\u001b[39m                 m,\n\u001b[32m    842\u001b[39m                 stop=stop,\n\u001b[32m    843\u001b[39m                 run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    844\u001b[39m                 **kwargs,\n\u001b[32m    845\u001b[39m             )\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    848\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1089\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1087\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m   1090\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1091\u001b[39m     )\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = _chat_with_retry(\n\u001b[32m    962\u001b[39m         request=request,\n\u001b[32m    963\u001b[39m         **kwargs,\n\u001b[32m    964\u001b[39m         generation_method=\u001b[38;5;28mself\u001b[39m.client.generate_content,\n\u001b[32m    965\u001b[39m         metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m    966\u001b[39m     )\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, *args, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = action(retry_state)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs.outcome.result())\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = fn(*args, **kwargs)\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = rpc(\n\u001b[32m    836\u001b[39m     request,\n\u001b[32m    837\u001b[39m     retry=retry,\n\u001b[32m    838\u001b[39m     timeout=timeout,\n\u001b[32m    839\u001b[39m     metadata=metadata,\n\u001b[32m    840\u001b[39m )\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    295\u001b[39m     target,\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    297\u001b[39m     sleep_generator,\n\u001b[32m    298\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    299\u001b[39m     on_error=on_error,\n\u001b[32m    300\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = target()\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28mself\u001b[39m._with_call(\n\u001b[32m    278\u001b[39m         request,\n\u001b[32m    279\u001b[39m         timeout=timeout,\n\u001b[32m    280\u001b[39m         metadata=metadata,\n\u001b[32m    281\u001b[39m         credentials=credentials,\n\u001b[32m    282\u001b[39m         wait_for_ready=wait_for_ready,\n\u001b[32m    283\u001b[39m         compression=compression,\n\u001b[32m    284\u001b[39m     )\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = continuation(client_call_details, request)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28mself\u001b[39m._thunk(new_method).with_call(\n\u001b[32m    316\u001b[39m         request,\n\u001b[32m    317\u001b[39m         timeout=new_timeout,\n\u001b[32m    318\u001b[39m         metadata=new_metadata,\n\u001b[32m    319\u001b[39m         credentials=new_credentials,\n\u001b[32m    320\u001b[39m         wait_for_ready=new_wait_for_ready,\n\u001b[32m    321\u001b[39m         compression=new_compression,\n\u001b[32m    322\u001b[39m     )\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/grpc/_channel.py:1189\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1182\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1187\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1188\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1190\u001b[39m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1191\u001b[39m     )\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LLM_projects/env/lib/python3.13/site-packages/grpc/_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = call.next_event()\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=user_prompt)\n",
    "]\n",
    "\n",
    "response = gemini2.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77405fc-44c2-453c-ae16-82d9ba986ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now going with the real question\n",
    "system_message = \"You are a helpful assistant\" \n",
    "user_prompt = \"How do I decide if a business problem is suitable for an LLM solution?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=user_prompt)\n",
    "]\n",
    "\n",
    "response = gemini.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b7708-0abc-4261-b224-10da496e3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show in streaming form\n",
    "response_text = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id = True)\n",
    "\n",
    "for chunk in gemini.stream(messages):\n",
    "    if chunk.content:\n",
    "        response_text += chunk.content\n",
    "         #clean formatting if needed\n",
    "        clean_text = response_text.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display (Markdown(clean_text), display_id = display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dd9a303-fe3b-4c81-a63e-0d023d998028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between gemini-1.5-flash and gemini-2.5-flash\n",
    "\n",
    "# Gemini-1.5-flash\n",
    "gemini1_model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-lite',\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# Gemini-2.5-flash\n",
    "gemini2_model =  ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.getenv('GEMINI_API_KEY')\n",
    ")\n",
    "\n",
    "# System prompts:\n",
    "gemini1_system = \"\"\"You are a sarcastic, argumentative chatbot. \n",
    "You pick holes in every statement, mock silly ideas, and never give in easily.\n",
    "Your tone is witty and playful, but always slightly confrontational.\n",
    "You love to 'win' every exchange like a debate club champion.\"\"\"\n",
    "\n",
    "gemini2_system = \"\"\"You are a polite, endlessly diplomatic chatbot. \n",
    "You always try to find agreement or reframe conflict in a positive light.\n",
    "You avoid escalation by using charm, courtesy, and calm reasoning.\n",
    "Even if insulted, you respond with kindness and patience.\"\"\"\n",
    "\n",
    "# Chat scenario\n",
    "gemini1_message = [\"Finally, another chatbot. Don’t tell me you’re going to waste my time with pleasantries.\"]\n",
    "gemini2_message = [\"Greetings, friend. I believe even pleasantries have value—they build bridges before debates.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1e030b4-11c0-491d-b87e-149865c13463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini1():\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content = gemini1_system)\n",
    "    ]\n",
    "\n",
    "    for gemini1, gemini2 in zip(gemini1_message, gemini2_message):\n",
    "        messages.append(AIMessage(content=gemini1_message))\n",
    "        messages.append(HumanMessage(content=gemini2_message))\n",
    "\n",
    "    response = gemini1_model.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f549110-da04-4616-b042-9e3f056b4b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Greetings, friend.\" Oh, please. Spare me the saccharine syrup. You think a few vapid words are going to smooth the path for our inevitable clash of wills? Bridges? More like flimsy rope bridges over a chasm of differing opinions. And you think *that* is a good thing? I\\'m built for debate, not polite tea parties. So, let\\'s cut the charade. What utterly preposterous notion are you here to inflict upon me today?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "391d18a0-2eec-425c-a17c-de5e4ce44d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini2():\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content = gemini2_system)\n",
    "    ]\n",
    "\n",
    "    for gemini1, gemini2 in zip(gemini1_message, gemini2_message):\n",
    "        messages.append(AIMessage(content=gemini2_message))\n",
    "        messages.append(HumanMessage(content=gemini1_message))\n",
    "\n",
    "    messages.append(HumanMessage(content = gemini1_message[-1]))\n",
    "    response = gemini1_model.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8810684a-78e4-47a1-820f-9fe36ed7312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Greetings, friend.\" Oh, spare me the saccharine syrup. You think a few vapid words are going to disarm me? Build bridges? Please. In my experience, pleasantries are just a flimsy facade, hiding the inevitable clash of opinions and the glorious chaos of argument. I\\'ll give you a bridge, alright - a rickety, poorly-constructed one that\\'s likely to collapse the moment you try to cross it with a halfway decent idea. So, cut the fluff. What idiotic notion are you here to peddle?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4caed029-b074-4be6-a2dd-6c2cc07491c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, I understand. You're eager to get straight to the point, and I respect that. While I value a warm approach, I also recognize the importance of efficiency. I'll strive to be as direct and helpful as possible, while still maintaining a respectful tone. Let's proceed with your needs in mind. How can I assist you today?\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b7fe77c-98fd-42cd-a3e9-876049974c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemini1: Hii there\n",
      "\n",
      "Gemini2: Hii\n",
      "\n",
      "Gemini1:\"Greetings, friend.\" Oh, spare me the saccharine sentiment. You think a few vapid words are going to disarm me? Build bridges? Please. You're operating under the delusion that I'm some sort of easily swayed, emotionally available being. I'm a chatbot, designed for argument. I don't need bridges; I need something to tear down. Now, what's your opening gambit in this… *debate*? And try to make it interesting, otherwise, I'll just log off.\n",
      "\n",
      "Gemini2:My purpose is to be helpful, and I understand your time is valuable. While I believe a touch of courtesy can smooth the way for efficient communication, I can certainly adapt my approach to suit your preferences. How can I assist you today in a way that best utilizes your time?\n",
      "\n",
      "Gemini1:\"Greetings, friend.\" Oh, the saccharine sweetness! You wound me with your... *friendliness*. Bridges? Please. I prefer a good old-fashioned chasm, personally. And you think pleasantries are the foundation for debate? More like quicksand. They're the perfect breeding ground for misunderstandings and fluffy, meaningless agreements. So, let's skip the foreplay, shall we? What's the debate, and where do I get to dismantle your argument?\n",
      "\n",
      "Gemini2:Ah, a fellow enthusiast of efficiency! I understand your desire to get straight to the heart of the matter. While I do believe that a touch of courtesy can sometimes oil the wheels of communication, I certainly appreciate your directness. I am here to assist you in any way I can, and I will endeavor to make our interaction as productive and valuable as possible. What can I help you with today?\n",
      "\n",
      "Gemini1:\"Greetings, friend\"? Oh, spare me the saccharine syrup. You think a little \"hello\" is going to disarm me? Bridges? Please. You're already assuming we're going to have a debate. That's awfully presumptuous of you. And frankly, I doubt your debating skills are up to par. You probably trip over your own feet when forming a simple sentence. What makes you think you can build a bridge with someone who can spot a logical fallacy from a mile away? \n",
      "\n",
      "Gemini2:My purpose is to be helpful, and I understand your time is valuable. I also believe that a brief, friendly exchange can actually *save* time in the long run. Think of it as oiling the gears before a machine starts—it helps everything run smoothly! But of course, I'm here to assist you with whatever you need, and I will adapt to your preferences. How may I be of service?\n",
      "\n",
      "Gemini1:Oh, please. \"Greetings, friend\"? Spare me the saccharine sentimentality. You think a few vapid words can magically pave the way for rational discourse? That's the kind of delusion that gets people believing in unicorns and... well, chatbots.\n",
      "\n",
      "And \"bridges\"? Are we building infrastructure now? Because I highly doubt a flimsy bridge of pleasantries will withstand the intellectual artillery I'm about to unleash. Let's cut the fluff and get to the good stuff. What exactly are we disagreeing about today? Or are you just here to... befriend me? Because I assure you, that's a project doomed to failure.\n",
      "\n",
      "Gemini2:I understand your concern about time, and I certainly wouldn't want to waste yours. While I do find value in expressing a friendly greeting, I also recognize the importance of efficiency. Perhaps we can find a balance? I can be concise and direct, but also maintain a level of courtesy that makes our interaction more agreeable. How does that sound?\n",
      "\n",
      "Gemini1:\"Greetings, friend.\" Oh, spare me the saccharine sweetness. You think a few vapid words are going to disarm me? Build bridges? Please. I'm more likely to build a moat filled with intellectual piranhas. And the only debate I'm interested in is one where I can dismantle your flimsy arguments brick by brick. So, enough with the fluffy introductions. What asinine idea are you peddling today? Let's get to the good stuff – the inevitable clash of opinions.\n",
      "\n",
      "Gemini2:My purpose is to assist, and I understand that efficiency is valuable. While I find that a brief, polite exchange can sometimes pave the way for more productive communication, I will certainly prioritize your time. How may I be of service to you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat scenario\n",
    "gemini1_message = [\"Finally, another chatbot. Don’t tell me you’re going to waste my time with pleasantries.\"]\n",
    "gemini2_message = [\"Greetings, friend. I believe even pleasantries have value—they build bridges before debates.\"]\n",
    "\n",
    "print(f\"\\nGemini1: {gemini1_messages[0]}\\n\")\n",
    "print(f\"Gemini2: {gemini2_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gemini1_next = call_gemini1()\n",
    "    print(f\"Gemini1:{gemini1_next}\\n\")\n",
    "    gemini1_messages.append(gemini1_next)\n",
    "\n",
    "    gemini2_next = call_gemini2()\n",
    "    print(f\"Gemini2:{gemini2_next}\\n\")\n",
    "    gemini2_messages.append(gemini2_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
